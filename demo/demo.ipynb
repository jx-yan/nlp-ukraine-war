{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet = \"they went to war when russia invaded ukraine they believed in freedom the right to live in their godgiven land they laid down their lives not only for ukraine but for all of europe rip mykhailo lompas 31 mykola los liubomyr lutsak roman lutsyk 28 \"\n",
    "hashtags = [\"ArmUkraineNow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tweet_processing(tweet,hashtags):\n",
    "    import contractions\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer \n",
    "    from gensim.parsing.preprocessing import STOPWORDS\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = new_tweet.split(\" \")\n",
    "    sentence = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            word = contractions.fix(word)\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            sentence.append(word)\n",
    "    processed = \" \".join(sentence)\n",
    "\n",
    "    processed = \" \".join([w.lower() for w in processed.split(\" \")])\n",
    "\n",
    "    processed = re.sub(r\"@\\S+\",\"\",processed)\n",
    "    processed = re.sub(r\"#\\S+\",\"\",processed)\n",
    "    processed = re.sub(\" amp \",\" \",processed)\n",
    "    processed = re.sub(r\"http\\S+\",\"\",processed)\n",
    "    processed = re.sub(r'[^\\w\\s]', '', processed)\n",
    "    processed = re.sub(r'[^a-zA-Z\\s]', '', processed)\n",
    "    processed = re.sub(r'\\d+', '', processed)\n",
    "\n",
    "    processed = word_tokenize(processed)\n",
    "\n",
    "    processed = [lemmatizer.lemmatize(token) for token in processed if token not in STOPWORDS and len(token) > 1]\n",
    "\n",
    "    processed = [w for w in processed if re.search('^[a-z]+$',w)]\n",
    "    \n",
    "    import pickle as pkl\n",
    "\n",
    "    with open(\"final_lda_model.pkl\", \"rb\") as fp:\n",
    "        lda_model = pkl.load(fp)\n",
    "    with open(\"dictionary.pkl\", \"rb\") as fp:\n",
    "        unigram_dictionary = pkl.load(fp)\n",
    "    with open(\"relevance_model.pkl\", \"rb\") as fp:\n",
    "        relevance_model = pkl.load(fp)\n",
    "    with open(\"sentiment_model.pkl\", \"rb\") as fp:\n",
    "        sentiment_model = pkl.load(fp)\n",
    "        \n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "\n",
    "    gensim_corpus = unigram_dictionary.doc2bow(processed)\n",
    "    topic_score = lda_model.get_document_topics(gensim_corpus)\n",
    "    max = 0\n",
    "    best = None\n",
    "    for i in range(0,len(topic_score)):\n",
    "        if topic_score[i][1] > max:\n",
    "            max = topic_score[i][1]\n",
    "            best = topic_score[i][0]\n",
    "\n",
    "    if best == 0:\n",
    "        best = \"Military Actions and Operations\"\n",
    "    elif best == 1:\n",
    "        best = \"Civilian Support and Information Sharing\"\n",
    "    elif best == 2:\n",
    "        best = \"Broad Overview of the Conflict\"\n",
    "    elif best == 3:\n",
    "        best = \"Strategic Military Aspects\"\n",
    "    else:\n",
    "        best = \"International Support and Public Opinion\"\n",
    "        \n",
    "    vector = unigram_dictionary.doc2bow(processed)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    for tag in hashtags:\n",
    "        sent_as_dict[tag] = 1\n",
    "        \n",
    "    relevance = relevance_model.classify(sent_as_dict)\n",
    "    \n",
    "    if relevance == \"Relevant\":\n",
    "        vector = unigram_dictionary.doc2bow(processed)\n",
    "        sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "        for tag in hashtags:\n",
    "            sent_as_dict[tag] = 1\n",
    "        sent_as_dict[best] = 1\n",
    "        sentiment = sentiment_model.classify(sent_as_dict)\n",
    "    else: \n",
    "        sentiment = \"Irrelevant\"\n",
    "        \n",
    "    from transformers import pipeline\n",
    "    import tensorflow\n",
    "\n",
    "    # download pre-trained emotion classification model\n",
    "    emorobertamodel = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "    \n",
    "    emotions = emorobertamodel(new_tweet)\n",
    "    \n",
    "    return (best,relevance,sentiment,emotions[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'tensorflow' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1472\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:63\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepth_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_question_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionPipeline\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\pipelines\\document_question_answering.py:29\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkPipeline, build_pipeline_init_args\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_starts_ends\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\data\\__init__.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     DataProcessor,\n\u001b[0;32m     29\u001b[0m     InputExample,\n\u001b[0;32m     30\u001b[0m     InputFeatures,\n\u001b[0;32m     31\u001b[0m     SingleSentenceClassificationProcessor,\n\u001b[0;32m     32\u001b[0m     SquadExample,\n\u001b[0;32m     33\u001b[0m     SquadFeatures,\n\u001b[0;32m     34\u001b[0m     SquadV1Processor,\n\u001b[0;32m     35\u001b[0m     SquadV2Processor,\n\u001b[0;32m     36\u001b[0m     glue_convert_examples_to_features,\n\u001b[0;32m     37\u001b[0m     glue_output_modes,\n\u001b[0;32m     38\u001b[0m     glue_processors,\n\u001b[0;32m     39\u001b[0m     glue_tasks_num_labels,\n\u001b[0;32m     40\u001b[0m     squad_convert_examples_to_features,\n\u001b[0;32m     41\u001b[0m     xnli_output_modes,\n\u001b[0;32m     42\u001b[0m     xnli_processors,\n\u001b[0;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\data\\processors\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msquad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\data\\processors\\glue.py:79\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tf_glue_convert_examples_to_features\u001b[39m(\n\u001b[1;32m---> 79\u001b[0m         examples: \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[0;32m     80\u001b[0m         tokenizer: PreTrainedTokenizer,\n\u001b[0;32m     81\u001b[0m         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     82\u001b[0m         max_length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     83\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m        Returns:\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m            A `tf.data.Dataset` containing the task-specific features.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (best,relevance,sentiment,emotions) \u001b[38;5;241m=\u001b[39m \u001b[43mtweet_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_tweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhashtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(best)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(relevance)\n",
      "Cell \u001b[1;32mIn[44], line 89\u001b[0m, in \u001b[0;36mtweet_processing\u001b[1;34m(tweet, hashtags)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m     87\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIrrelevant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# download pre-trained emotion classification model\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1462\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1462\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1463\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1474\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1475\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1476\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1477\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'tensorflow' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "(best,relevance,sentiment,emotions) = tweet_processing(new_tweet,hashtags)\n",
    "print(best)\n",
    "print(relevance)\n",
    "print(sentiment)\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
