{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3942b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "\n",
    "# stanfordnlp.download('en')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53cd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99zhe\\AppData\\Local\\Temp\\ipykernel_46416\\2020894419.py:1: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(\"uk_ru_2023_en_text_random_labeled_dataset_Labelled.csv\",encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"uk_ru_2023_en_text_random_labeled_dataset_Labelled.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871d9aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userid</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>token</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>Topic Weight</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.467372e+07</td>\n",
       "      <td>1.627620e+18</td>\n",
       "      <td>repdavid i support 100</td>\n",
       "      <td>SlavaUkraïni,JoeBiden</td>\n",
       "      <td>['repdavid', 'support']</td>\n",
       "      <td>repdavid support</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.389973</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.282984e+09</td>\n",
       "      <td>1.611400e+18</td>\n",
       "      <td>province editorial inequality hounds s populat...</td>\n",
       "      <td>Canada,disabled,Vancouver,homeless</td>\n",
       "      <td>['province', 'editorial', 'inequality', 'hound...</td>\n",
       "      <td>province editorial inequality hound population...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.464470</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.830324e+08</td>\n",
       "      <td>1.636650e+18</td>\n",
       "      <td>they went to war when russia invaded ukraine t...</td>\n",
       "      <td>ArmUkraineNow</td>\n",
       "      <td>['went', 'war', 'russia', 'invaded', 'ukraine'...</td>\n",
       "      <td>went war russia invaded ukraine believed freed...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.613352</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.257760e+18</td>\n",
       "      <td>1.626920e+18</td>\n",
       "      <td>on mintwave radio mint2dry</td>\n",
       "      <td>nowplaying,alexa,streema,Scotland,echo,global,...</td>\n",
       "      <td>['mintwave', 'radio', 'mintdry']</td>\n",
       "      <td>mintwave radio mintdry</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.241172e+08</td>\n",
       "      <td>1.640020e+18</td>\n",
       "      <td>military historian douglas macgregor why will ...</td>\n",
       "      <td>NATO,Russia,Kiev</td>\n",
       "      <td>['military', 'historian', 'douglas', 'macgrego...</td>\n",
       "      <td>military historian douglas macgregor crumble d...</td>\n",
       "      <td>Broad Overview of the Conflict</td>\n",
       "      <td>0.434376</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        userid       tweetid  \\\n",
       "0           0  5.467372e+07  1.627620e+18   \n",
       "1           1  2.282984e+09  1.611400e+18   \n",
       "2           2  2.830324e+08  1.636650e+18   \n",
       "3           3  1.257760e+18  1.626920e+18   \n",
       "4           4  9.241172e+08  1.640020e+18   \n",
       "\n",
       "                                                text  \\\n",
       "0                            repdavid i support 100    \n",
       "1  province editorial inequality hounds s populat...   \n",
       "2  they went to war when russia invaded ukraine t...   \n",
       "3                         on mintwave radio mint2dry   \n",
       "4  military historian douglas macgregor why will ...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                              SlavaUkraïni,JoeBiden   \n",
       "1                 Canada,disabled,Vancouver,homeless   \n",
       "2                                      ArmUkraineNow   \n",
       "3  nowplaying,alexa,streema,Scotland,echo,global,...   \n",
       "4                                   NATO,Russia,Kiev   \n",
       "\n",
       "                                               token  \\\n",
       "0                            ['repdavid', 'support']   \n",
       "1  ['province', 'editorial', 'inequality', 'hound...   \n",
       "2  ['went', 'war', 'russia', 'invaded', 'ukraine'...   \n",
       "3                   ['mintwave', 'radio', 'mintdry']   \n",
       "4  ['military', 'historian', 'douglas', 'macgrego...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0                                   repdavid support   \n",
       "1  province editorial inequality hound population...   \n",
       "2  went war russia invaded ukraine believed freed...   \n",
       "3                             mintwave radio mintdry   \n",
       "4  military historian douglas macgregor crumble d...   \n",
       "\n",
       "                                Topic Label  Topic Weight   Sentiment  \n",
       "0  International Support and Public Opinion      0.389973    Positive  \n",
       "1                Strategic Military Aspects      0.464470  Irrelevant  \n",
       "2                Strategic Military Aspects      0.613352    Positive  \n",
       "3           Military Actions and Operations      0.279435  Irrelevant  \n",
       "4            Broad Overview of the Conflict      0.434376    Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbcdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(x):\n",
    "    if x in [\"Positive\",\"Negative\",\"Neutral\"]:\n",
    "        return \"Relevant\"\n",
    "    else:\n",
    "        return \"Irrelevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdfe5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"Relevance\"] = tweets[\"Sentiment\"].apply(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caccd3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>token</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>Topic Weight</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.467372e+07</td>\n",
       "      <td>1.627620e+18</td>\n",
       "      <td>repdavid i support 100</td>\n",
       "      <td>SlavaUkraïni,JoeBiden</td>\n",
       "      <td>['repdavid', 'support']</td>\n",
       "      <td>repdavid support</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.389973</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.282984e+09</td>\n",
       "      <td>1.611400e+18</td>\n",
       "      <td>province editorial inequality hounds s populat...</td>\n",
       "      <td>Canada,disabled,Vancouver,homeless</td>\n",
       "      <td>['province', 'editorial', 'inequality', 'hound...</td>\n",
       "      <td>province editorial inequality hound population...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.464470</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.830324e+08</td>\n",
       "      <td>1.636650e+18</td>\n",
       "      <td>they went to war when russia invaded ukraine t...</td>\n",
       "      <td>ArmUkraineNow</td>\n",
       "      <td>['went', 'war', 'russia', 'invaded', 'ukraine'...</td>\n",
       "      <td>went war russia invaded ukraine believed freed...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.613352</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.257760e+18</td>\n",
       "      <td>1.626920e+18</td>\n",
       "      <td>on mintwave radio mint2dry</td>\n",
       "      <td>nowplaying,alexa,streema,Scotland,echo,global,...</td>\n",
       "      <td>['mintwave', 'radio', 'mintdry']</td>\n",
       "      <td>mintwave radio mintdry</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.241172e+08</td>\n",
       "      <td>1.640020e+18</td>\n",
       "      <td>military historian douglas macgregor why will ...</td>\n",
       "      <td>NATO,Russia,Kiev</td>\n",
       "      <td>['military', 'historian', 'douglas', 'macgrego...</td>\n",
       "      <td>military historian douglas macgregor crumble d...</td>\n",
       "      <td>Broad Overview of the Conflict</td>\n",
       "      <td>0.434376</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid       tweetid  \\\n",
       "0  5.467372e+07  1.627620e+18   \n",
       "1  2.282984e+09  1.611400e+18   \n",
       "2  2.830324e+08  1.636650e+18   \n",
       "3  1.257760e+18  1.626920e+18   \n",
       "4  9.241172e+08  1.640020e+18   \n",
       "\n",
       "                                                text  \\\n",
       "0                            repdavid i support 100    \n",
       "1  province editorial inequality hounds s populat...   \n",
       "2  they went to war when russia invaded ukraine t...   \n",
       "3                         on mintwave radio mint2dry   \n",
       "4  military historian douglas macgregor why will ...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                              SlavaUkraïni,JoeBiden   \n",
       "1                 Canada,disabled,Vancouver,homeless   \n",
       "2                                      ArmUkraineNow   \n",
       "3  nowplaying,alexa,streema,Scotland,echo,global,...   \n",
       "4                                   NATO,Russia,Kiev   \n",
       "\n",
       "                                               token  \\\n",
       "0                            ['repdavid', 'support']   \n",
       "1  ['province', 'editorial', 'inequality', 'hound...   \n",
       "2  ['went', 'war', 'russia', 'invaded', 'ukraine'...   \n",
       "3                   ['mintwave', 'radio', 'mintdry']   \n",
       "4  ['military', 'historian', 'douglas', 'macgrego...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0                                   repdavid support   \n",
       "1  province editorial inequality hound population...   \n",
       "2  went war russia invaded ukraine believed freed...   \n",
       "3                             mintwave radio mintdry   \n",
       "4  military historian douglas macgregor crumble d...   \n",
       "\n",
       "                                Topic Label  Topic Weight   Sentiment  \\\n",
       "0  International Support and Public Opinion      0.389973    Positive   \n",
       "1                Strategic Military Aspects      0.464470  Irrelevant   \n",
       "2                Strategic Military Aspects      0.613352    Positive   \n",
       "3           Military Actions and Operations      0.279435  Irrelevant   \n",
       "4            Broad Overview of the Conflict      0.434376    Negative   \n",
       "\n",
       "    Relevance  \n",
       "0    Relevant  \n",
       "1  Irrelevant  \n",
       "2    Relevant  \n",
       "3  Irrelevant  \n",
       "4    Relevant  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.iloc[: , 1:]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7058d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99zhe\\AppData\\Local\\Temp\\ipykernel_46416\\2160210669.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets.iloc[[i]][\"token\"] = sent\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,tweets.shape[0]):\n",
    "    sent = tweets.iloc[[i]][\"token\"]\n",
    "    sent = [lemmatizer.lemmatize(w) for w in sent]\n",
    "    tweets.iloc[[i]][\"token\"] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5419fa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userid            5000\n",
       "tweetid           5000\n",
       "text              5000\n",
       "hashtags          4975\n",
       "token             5000\n",
       "processed_text    4999\n",
       "Topic Label       5000\n",
       "Topic Weight      5000\n",
       "Sentiment         5000\n",
       "Relevance         5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_tweets = tweets[~tweets[\"Sentiment\"].isnull()]\n",
    "labelled_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b830297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>token</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>Topic Weight</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.467372e+07</td>\n",
       "      <td>1.627620e+18</td>\n",
       "      <td>repdavid i support 100</td>\n",
       "      <td>SlavaUkraïni,JoeBiden</td>\n",
       "      <td>['repdavid', 'support']</td>\n",
       "      <td>repdavid support</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.389973</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.282984e+09</td>\n",
       "      <td>1.611400e+18</td>\n",
       "      <td>province editorial inequality hounds s populat...</td>\n",
       "      <td>Canada,disabled,Vancouver,homeless</td>\n",
       "      <td>['province', 'editorial', 'inequality', 'hound...</td>\n",
       "      <td>province editorial inequality hound population...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.464470</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.830324e+08</td>\n",
       "      <td>1.636650e+18</td>\n",
       "      <td>they went to war when russia invaded ukraine t...</td>\n",
       "      <td>ArmUkraineNow</td>\n",
       "      <td>['went', 'war', 'russia', 'invaded', 'ukraine'...</td>\n",
       "      <td>went war russia invaded ukraine believed freed...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.613352</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.257760e+18</td>\n",
       "      <td>1.626920e+18</td>\n",
       "      <td>on mintwave radio mint2dry</td>\n",
       "      <td>nowplaying,alexa,streema,Scotland,echo,global,...</td>\n",
       "      <td>['mintwave', 'radio', 'mintdry']</td>\n",
       "      <td>mintwave radio mintdry</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.241172e+08</td>\n",
       "      <td>1.640020e+18</td>\n",
       "      <td>military historian douglas macgregor why will ...</td>\n",
       "      <td>NATO,Russia,Kiev</td>\n",
       "      <td>['military', 'historian', 'douglas', 'macgrego...</td>\n",
       "      <td>military historian douglas macgregor crumble d...</td>\n",
       "      <td>Broad Overview of the Conflict</td>\n",
       "      <td>0.434376</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>1.226100e+18</td>\n",
       "      <td>1.633460e+18</td>\n",
       "      <td>will be next we told you this wasnt about just...</td>\n",
       "      <td>Georgia,Moldova,Ukraine,Putin,Russia,RussiaIsa...</td>\n",
       "      <td>['told', 'wasnt', 'clear', 'want', 'restore', ...</td>\n",
       "      <td>told wasnt clear want restore root oppression</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.472631</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>1.496810e+18</td>\n",
       "      <td>1.636100e+18</td>\n",
       "      <td>a few good russians</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>['good', 'russian']</td>\n",
       "      <td>good russian</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.326523</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2.611555e+07</td>\n",
       "      <td>1.637140e+18</td>\n",
       "      <td>👏🏼👏🏼👏🏼we cant get enough military equipmentwea...</td>\n",
       "      <td>PutinIsaWarCriminal,FCKPUTIN,RussiaIsATerroris...</td>\n",
       "      <td>['military', 'equipmentweapons', 'guy', 'fast'...</td>\n",
       "      <td>military equipmentweapons guy fast slava ukrai...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.471844</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>1.274051e+09</td>\n",
       "      <td>1.634670e+18</td>\n",
       "      <td>a russian mtlb carrying 100mm ammunition for t...</td>\n",
       "      <td>Ukraine,Donetsk</td>\n",
       "      <td>['russian', 'mtlb', 'carrying', 'mm', 'ammunit...</td>\n",
       "      <td>russian mtlb carrying mm ammunition mt rapira ...</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.719004</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>1.573300e+18</td>\n",
       "      <td>1.658520e+18</td>\n",
       "      <td>leaders to mediate in conflict and have both a...</td>\n",
       "      <td>African,Ukraine,Putin,Zelensky,SouthAfrica</td>\n",
       "      <td>['leader', 'mediate', 'conflict', 'agreed', 'r...</td>\n",
       "      <td>leader mediate conflict agreed receive peace d...</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.408629</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            userid       tweetid  \\\n",
       "0     5.467372e+07  1.627620e+18   \n",
       "1     2.282984e+09  1.611400e+18   \n",
       "2     2.830324e+08  1.636650e+18   \n",
       "3     1.257760e+18  1.626920e+18   \n",
       "4     9.241172e+08  1.640020e+18   \n",
       "...            ...           ...   \n",
       "2995  1.226100e+18  1.633460e+18   \n",
       "2996  1.496810e+18  1.636100e+18   \n",
       "2997  2.611555e+07  1.637140e+18   \n",
       "2998  1.274051e+09  1.634670e+18   \n",
       "2999  1.573300e+18  1.658520e+18   \n",
       "\n",
       "                                                   text  \\\n",
       "0                               repdavid i support 100    \n",
       "1     province editorial inequality hounds s populat...   \n",
       "2     they went to war when russia invaded ukraine t...   \n",
       "3                            on mintwave radio mint2dry   \n",
       "4     military historian douglas macgregor why will ...   \n",
       "...                                                 ...   \n",
       "2995  will be next we told you this wasnt about just...   \n",
       "2996                               a few good russians    \n",
       "2997  👏🏼👏🏼👏🏼we cant get enough military equipmentwea...   \n",
       "2998  a russian mtlb carrying 100mm ammunition for t...   \n",
       "2999  leaders to mediate in conflict and have both a...   \n",
       "\n",
       "                                               hashtags  \\\n",
       "0                                 SlavaUkraïni,JoeBiden   \n",
       "1                    Canada,disabled,Vancouver,homeless   \n",
       "2                                         ArmUkraineNow   \n",
       "3     nowplaying,alexa,streema,Scotland,echo,global,...   \n",
       "4                                      NATO,Russia,Kiev   \n",
       "...                                                 ...   \n",
       "2995  Georgia,Moldova,Ukraine,Putin,Russia,RussiaIsa...   \n",
       "2996                                            Ukraine   \n",
       "2997  PutinIsaWarCriminal,FCKPUTIN,RussiaIsATerroris...   \n",
       "2998                                    Ukraine,Donetsk   \n",
       "2999         African,Ukraine,Putin,Zelensky,SouthAfrica   \n",
       "\n",
       "                                                  token  \\\n",
       "0                               ['repdavid', 'support']   \n",
       "1     ['province', 'editorial', 'inequality', 'hound...   \n",
       "2     ['went', 'war', 'russia', 'invaded', 'ukraine'...   \n",
       "3                      ['mintwave', 'radio', 'mintdry']   \n",
       "4     ['military', 'historian', 'douglas', 'macgrego...   \n",
       "...                                                 ...   \n",
       "2995  ['told', 'wasnt', 'clear', 'want', 'restore', ...   \n",
       "2996                                ['good', 'russian']   \n",
       "2997  ['military', 'equipmentweapons', 'guy', 'fast'...   \n",
       "2998  ['russian', 'mtlb', 'carrying', 'mm', 'ammunit...   \n",
       "2999  ['leader', 'mediate', 'conflict', 'agreed', 'r...   \n",
       "\n",
       "                                         processed_text  \\\n",
       "0                                      repdavid support   \n",
       "1     province editorial inequality hound population...   \n",
       "2     went war russia invaded ukraine believed freed...   \n",
       "3                                mintwave radio mintdry   \n",
       "4     military historian douglas macgregor crumble d...   \n",
       "...                                                 ...   \n",
       "2995      told wasnt clear want restore root oppression   \n",
       "2996                                       good russian   \n",
       "2997  military equipmentweapons guy fast slava ukrai...   \n",
       "2998  russian mtlb carrying mm ammunition mt rapira ...   \n",
       "2999  leader mediate conflict agreed receive peace d...   \n",
       "\n",
       "                                   Topic Label  Topic Weight   Sentiment  \\\n",
       "0     International Support and Public Opinion      0.389973    Positive   \n",
       "1                   Strategic Military Aspects      0.464470  Irrelevant   \n",
       "2                   Strategic Military Aspects      0.613352    Positive   \n",
       "3              Military Actions and Operations      0.279435  Irrelevant   \n",
       "4               Broad Overview of the Conflict      0.434376    Negative   \n",
       "...                                        ...           ...         ...   \n",
       "2995           Military Actions and Operations      0.472631    Negative   \n",
       "2996           Military Actions and Operations      0.326523    Positive   \n",
       "2997                Strategic Military Aspects      0.471844    Positive   \n",
       "2998           Military Actions and Operations      0.719004     Neutral   \n",
       "2999  International Support and Public Opinion      0.408629     Neutral   \n",
       "\n",
       "       Relevance  \n",
       "0       Relevant  \n",
       "1     Irrelevant  \n",
       "2       Relevant  \n",
       "3     Irrelevant  \n",
       "4       Relevant  \n",
       "...          ...  \n",
       "2995    Relevant  \n",
       "2996    Relevant  \n",
       "2997    Relevant  \n",
       "2998    Relevant  \n",
       "2999    Relevant  \n",
       "\n",
       "[3000 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_tweets.iloc[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed0d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = labelled_tweets.iloc[0:3000]\n",
    "test_tweets = labelled_tweets.iloc[3000:4000]\n",
    "validate_tweets = labelled_tweets.iloc[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae2b70",
   "metadata": {},
   "source": [
    "# Identifying Relevant and Irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b93da",
   "metadata": {},
   "source": [
    "With Topic and Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36572547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.741\n",
      "             2          -0.30649        0.852\n",
      "             3          -0.23281        0.937\n",
      "             4          -0.19160        0.967\n",
      "             5          -0.16453        0.975\n",
      "             6          -0.14504        0.981\n",
      "             7          -0.13018        0.985\n",
      "             8          -0.11840        0.988\n",
      "             9          -0.10877        0.989\n",
      "            10          -0.10073        0.990\n",
      "            11          -0.09391        0.991\n",
      "            12          -0.08803        0.992\n",
      "            13          -0.08291        0.993\n",
      "            14          -0.07840        0.994\n",
      "            15          -0.07439        0.995\n",
      "            16          -0.07081        0.995\n",
      "            17          -0.06759        0.995\n",
      "            18          -0.06468        0.995\n",
      "            19          -0.06202        0.995\n",
      "            20          -0.05960        0.996\n",
      "            21          -0.05737        0.996\n",
      "            22          -0.05532        0.996\n",
      "            23          -0.05343        0.996\n",
      "            24          -0.05167        0.996\n",
      "            25          -0.05004        0.996\n",
      "            26          -0.04852        0.996\n",
      "            27          -0.04709        0.996\n",
      "            28          -0.04576        0.996\n",
      "            29          -0.04451        0.996\n",
      "            30          -0.04333        0.996\n",
      "            31          -0.04222        0.996\n",
      "            32          -0.04117        0.996\n",
      "            33          -0.04018        0.996\n",
      "            34          -0.03924        0.996\n",
      "            35          -0.03835        0.996\n",
      "            36          -0.03750        0.996\n",
      "            37          -0.03670        0.996\n",
      "            38          -0.03593        0.996\n",
      "            39          -0.03520        0.996\n",
      "            40          -0.03450        0.996\n",
      "            41          -0.03383        0.996\n",
      "            42          -0.03319        0.996\n",
      "            43          -0.03258        0.996\n",
      "            44          -0.03199        0.996\n",
      "            45          -0.03143        0.996\n",
      "            46          -0.03089        0.996\n",
      "            47          -0.03037        0.996\n",
      "            48          -0.02987        0.996\n",
      "            49          -0.02939        0.996\n",
      "            50          -0.02892        0.996\n",
      "            51          -0.02848        0.996\n",
      "            52          -0.02804        0.996\n",
      "            53          -0.02763        0.996\n",
      "            54          -0.02723        0.996\n",
      "            55          -0.02684        0.996\n",
      "            56          -0.02646        0.996\n",
      "            57          -0.02610        0.996\n",
      "            58          -0.02574        0.996\n",
      "            59          -0.02540        0.996\n",
      "            60          -0.02507        0.996\n",
      "            61          -0.02475        0.996\n",
      "            62          -0.02444        0.996\n",
      "            63          -0.02414        0.996\n",
      "            64          -0.02384        0.996\n",
      "            65          -0.02356        0.996\n",
      "            66          -0.02328        0.997\n",
      "            67          -0.02301        0.997\n",
      "            68          -0.02275        0.997\n",
      "            69          -0.02250        0.997\n",
      "            70          -0.02225        0.997\n",
      "            71          -0.02201        0.997\n",
      "            72          -0.02177        0.997\n",
      "            73          -0.02154        0.997\n",
      "            74          -0.02132        0.997\n",
      "            75          -0.02110        0.997\n",
      "            76          -0.02088        0.997\n",
      "            77          -0.02068        0.997\n",
      "            78          -0.02047        0.997\n",
      "            79          -0.02027        0.997\n",
      "            80          -0.02008        0.997\n",
      "            81          -0.01989        0.997\n",
      "            82          -0.01971        0.997\n",
      "            83          -0.01953        0.997\n",
      "            84          -0.01935        0.997\n",
      "            85          -0.01918        0.997\n",
      "            86          -0.01901        0.997\n",
      "            87          -0.01884        0.997\n",
      "            88          -0.01868        0.997\n",
      "            89          -0.01852        0.997\n",
      "            90          -0.01837        0.997\n",
      "            91          -0.01821        0.997\n",
      "            92          -0.01806        0.997\n",
      "            93          -0.01792        0.997\n",
      "            94          -0.01778        0.997\n",
      "            95          -0.01764        0.997\n",
      "            96          -0.01750        0.997\n",
      "            97          -0.01736        0.997\n",
      "            98          -0.01723        0.997\n",
      "            99          -0.01710        0.997\n",
      "         Final          -0.01697        0.997\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Topic and Hashtags\n",
    "\n",
    "train_labels = train_tweets[\"Relevance\"].tolist()\n",
    "train_hashtags = train_tweets[\"hashtags\"].tolist()\n",
    "train_hashtags = [str(hashtags).split(\",\") for hashtags in train_hashtags] \n",
    "train_topics = train_tweets[\"Topic Label\"].tolist()\n",
    "train_corpus = list(train_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s, t, h) in zip(train_labels, train_corpus, train_topics, train_hashtags):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifierRelevance1NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifierRelevance1M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d0ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy on Relevance test data:  0.647\n",
      "M Accuracy on Relevance test data:  0.769\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Topic and Hashtags\n",
    "\n",
    "test_labels = test_tweets[\"Relevance\"].tolist()\n",
    "test_hashtags = test_tweets[\"hashtags\"].tolist()\n",
    "test_hashtags = [str(hashtags).split(\",\") for hashtags in test_hashtags] \n",
    "test_topics = test_tweets[\"Topic Label\"].tolist()\n",
    "test_corpus = list(test_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, t, h) in zip(test_labels, test_corpus, test_topics, test_hashtags):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[\"t\"] = 1\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Accuracy on Relevance test data: \", nltk.classify.accuracy(classifierRelevance1NB, labeled_test_data))\n",
    "versionRelevance1NBacc = nltk.classify.accuracy(classifierRelevance1NB, labeled_test_data)\n",
    "print(\"M Accuracy on Relevance test data: \", nltk.classify.accuracy(classifierRelevance1M, labeled_test_data))\n",
    "versionRelevance1Macc = nltk.classify.accuracy(classifierRelevance1M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e57fa5",
   "metadata": {},
   "source": [
    "With Hashtags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c8a6e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.741\n",
      "             2          -0.29599        0.864\n",
      "             3          -0.22338        0.942\n",
      "             4          -0.18341        0.968\n",
      "             5          -0.15728        0.978\n",
      "             6          -0.13852        0.983\n",
      "             7          -0.12423        0.986\n",
      "             8          -0.11290        0.988\n",
      "             9          -0.10366        0.990\n",
      "            10          -0.09595        0.991\n",
      "            11          -0.08941        0.991\n",
      "            12          -0.08378        0.994\n",
      "            13          -0.07888        0.994\n",
      "            14          -0.07457        0.995\n",
      "            15          -0.07075        0.995\n",
      "            16          -0.06733        0.995\n",
      "            17          -0.06425        0.995\n",
      "            18          -0.06147        0.995\n",
      "            19          -0.05895        0.996\n",
      "            20          -0.05664        0.996\n",
      "            21          -0.05452        0.996\n",
      "            22          -0.05257        0.996\n",
      "            23          -0.05077        0.996\n",
      "            24          -0.04910        0.996\n",
      "            25          -0.04755        0.996\n",
      "            26          -0.04610        0.996\n",
      "            27          -0.04475        0.996\n",
      "            28          -0.04348        0.996\n",
      "            29          -0.04229        0.996\n",
      "            30          -0.04117        0.996\n",
      "            31          -0.04012        0.996\n",
      "            32          -0.03913        0.996\n",
      "            33          -0.03819        0.996\n",
      "            34          -0.03730        0.996\n",
      "            35          -0.03645        0.996\n",
      "            36          -0.03565        0.996\n",
      "            37          -0.03489        0.996\n",
      "            38          -0.03416        0.996\n",
      "            39          -0.03347        0.996\n",
      "            40          -0.03280        0.996\n",
      "            41          -0.03217        0.996\n",
      "            42          -0.03157        0.996\n",
      "            43          -0.03099        0.996\n",
      "            44          -0.03043        0.996\n",
      "            45          -0.02990        0.996\n",
      "            46          -0.02939        0.996\n",
      "            47          -0.02890        0.996\n",
      "            48          -0.02842        0.996\n",
      "            49          -0.02797        0.996\n",
      "            50          -0.02753        0.996\n",
      "            51          -0.02711        0.996\n",
      "            52          -0.02670        0.996\n",
      "            53          -0.02630        0.996\n",
      "            54          -0.02592        0.996\n",
      "            55          -0.02556        0.996\n",
      "            56          -0.02520        0.996\n",
      "            57          -0.02486        0.996\n",
      "            58          -0.02452        0.996\n",
      "            59          -0.02420        0.996\n",
      "            60          -0.02389        0.996\n",
      "            61          -0.02359        0.996\n",
      "            62          -0.02329        0.996\n",
      "            63          -0.02301        0.996\n",
      "            64          -0.02273        0.996\n",
      "            65          -0.02246        0.996\n",
      "            66          -0.02220        0.996\n",
      "            67          -0.02194        0.996\n",
      "            68          -0.02169        0.996\n",
      "            69          -0.02145        0.996\n",
      "            70          -0.02122        0.996\n",
      "            71          -0.02099        0.996\n",
      "            72          -0.02077        0.997\n",
      "            73          -0.02055        0.997\n",
      "            74          -0.02034        0.997\n",
      "            75          -0.02013        0.997\n",
      "            76          -0.01993        0.997\n",
      "            77          -0.01973        0.997\n",
      "            78          -0.01954        0.997\n",
      "            79          -0.01935        0.997\n",
      "            80          -0.01917        0.997\n",
      "            81          -0.01899        0.997\n",
      "            82          -0.01882        0.997\n",
      "            83          -0.01865        0.997\n",
      "            84          -0.01848        0.997\n",
      "            85          -0.01832        0.997\n",
      "            86          -0.01816        0.997\n",
      "            87          -0.01800        0.997\n",
      "            88          -0.01785        0.997\n",
      "            89          -0.01770        0.997\n",
      "            90          -0.01755        0.997\n",
      "            91          -0.01741        0.997\n",
      "            92          -0.01727        0.997\n",
      "            93          -0.01713        0.997\n",
      "            94          -0.01699        0.997\n",
      "            95          -0.01686        0.997\n",
      "            96          -0.01673        0.997\n",
      "            97          -0.01660        0.997\n",
      "            98          -0.01648        0.997\n",
      "            99          -0.01636        0.997\n",
      "         Final          -0.01623        0.997\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Hashtags\n",
    "\n",
    "train_labels = train_tweets[\"Relevance\"].tolist()\n",
    "train_hashtags = train_tweets[\"hashtags\"].tolist()\n",
    "train_hashtags = [str(hashtags).split(\",\") for hashtags in train_hashtags] \n",
    "train_corpus = list(train_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s, h) in zip(train_labels, train_corpus, train_hashtags):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifierRelevance2NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifierRelevance2M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16fab27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Relevance Accuracy on test data:  0.706\n",
      "M Relevance Accuracy on test data:  0.781\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Hashtags\n",
    "\n",
    "test_labels = test_tweets[\"Relevance\"].tolist()\n",
    "test_hashtags = test_tweets[\"hashtags\"].tolist()\n",
    "test_hashtags = [str(hashtags).split(\",\") for hashtags in test_hashtags] \n",
    "test_corpus = list(test_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, h) in zip(test_labels, test_corpus, test_hashtags):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Relevance Accuracy on test data: \", nltk.classify.accuracy(classifierRelevance2NB, labeled_test_data))\n",
    "versionRelevance2NBacc = nltk.classify.accuracy(classifierRelevance2NB, labeled_test_data)\n",
    "print(\"M Relevance Accuracy on test data: \", nltk.classify.accuracy(classifierRelevance2M, labeled_test_data))\n",
    "versionRelevance2Macc = nltk.classify.accuracy(classifierRelevance2M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03cf74",
   "metadata": {},
   "source": [
    "With Topic only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6ef3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.741\n",
      "             2          -0.33183        0.822\n",
      "             3          -0.26392        0.918\n",
      "             4          -0.22381        0.949\n",
      "             5          -0.19653        0.961\n",
      "             6          -0.17637        0.971\n",
      "             7          -0.16067        0.977\n",
      "             8          -0.14799        0.981\n",
      "             9          -0.13747        0.984\n",
      "            10          -0.12856        0.985\n",
      "            11          -0.12091        0.987\n",
      "            12          -0.11424        0.988\n",
      "            13          -0.10836        0.989\n",
      "            14          -0.10314        0.989\n",
      "            15          -0.09847        0.990\n",
      "            16          -0.09426        0.990\n",
      "            17          -0.09044        0.990\n",
      "            18          -0.08696        0.990\n",
      "            19          -0.08377        0.991\n",
      "            20          -0.08084        0.992\n",
      "            21          -0.07813        0.992\n",
      "            22          -0.07563        0.993\n",
      "            23          -0.07330        0.994\n",
      "            24          -0.07113        0.994\n",
      "            25          -0.06910        0.994\n",
      "            26          -0.06720        0.994\n",
      "            27          -0.06542        0.994\n",
      "            28          -0.06374        0.995\n",
      "            29          -0.06215        0.995\n",
      "            30          -0.06066        0.995\n",
      "            31          -0.05925        0.995\n",
      "            32          -0.05791        0.995\n",
      "            33          -0.05663        0.995\n",
      "            34          -0.05543        0.995\n",
      "            35          -0.05428        0.995\n",
      "            36          -0.05318        0.995\n",
      "            37          -0.05213        0.995\n",
      "            38          -0.05113        0.995\n",
      "            39          -0.05018        0.995\n",
      "            40          -0.04926        0.995\n",
      "            41          -0.04838        0.995\n",
      "            42          -0.04754        0.995\n",
      "            43          -0.04673        0.995\n",
      "            44          -0.04596        0.995\n",
      "            45          -0.04521        0.995\n",
      "            46          -0.04449        0.995\n",
      "            47          -0.04380        0.995\n",
      "            48          -0.04313        0.995\n",
      "            49          -0.04249        0.995\n",
      "            50          -0.04187        0.995\n",
      "            51          -0.04126        0.995\n",
      "            52          -0.04068        0.995\n",
      "            53          -0.04012        0.995\n",
      "            54          -0.03958        0.995\n",
      "            55          -0.03905        0.995\n",
      "            56          -0.03854        0.995\n",
      "            57          -0.03805        0.995\n",
      "            58          -0.03757        0.995\n",
      "            59          -0.03710        0.995\n",
      "            60          -0.03665        0.995\n",
      "            61          -0.03621        0.995\n",
      "            62          -0.03579        0.995\n",
      "            63          -0.03537        0.995\n",
      "            64          -0.03497        0.995\n",
      "            65          -0.03458        0.995\n",
      "            66          -0.03419        0.995\n",
      "            67          -0.03382        0.995\n",
      "            68          -0.03346        0.995\n",
      "            69          -0.03311        0.995\n",
      "            70          -0.03276        0.995\n",
      "            71          -0.03243        0.995\n",
      "            72          -0.03210        0.995\n",
      "            73          -0.03178        0.995\n",
      "            74          -0.03147        0.995\n",
      "            75          -0.03116        0.995\n",
      "            76          -0.03087        0.995\n",
      "            77          -0.03058        0.995\n",
      "            78          -0.03029        0.995\n",
      "            79          -0.03002        0.995\n",
      "            80          -0.02975        0.995\n",
      "            81          -0.02948        0.995\n",
      "            82          -0.02922        0.995\n",
      "            83          -0.02897        0.995\n",
      "            84          -0.02872        0.995\n",
      "            85          -0.02848        0.995\n",
      "            86          -0.02824        0.995\n",
      "            87          -0.02800        0.995\n",
      "            88          -0.02777        0.996\n",
      "            89          -0.02755        0.996\n",
      "            90          -0.02733        0.996\n",
      "            91          -0.02712        0.996\n",
      "            92          -0.02691        0.996\n",
      "            93          -0.02670        0.996\n",
      "            94          -0.02649        0.996\n",
      "            95          -0.02630        0.996\n",
      "            96          -0.02610        0.996\n",
      "            97          -0.02591        0.996\n",
      "            98          -0.02572        0.996\n",
      "            99          -0.02554        0.996\n",
      "         Final          -0.02535        0.996\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Topic\n",
    "\n",
    "train_labels = train_tweets[\"Relevance\"].tolist()\n",
    "train_topics = train_tweets[\"Topic Label\"].tolist()\n",
    "train_corpus = list(train_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s, t) in zip(train_labels, train_corpus, train_topics):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifierRelevance3NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifierRelevance3M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12b97019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Relevance Accuracy on test data:  0.543\n",
      "M Relevance Accuracy on test data:  0.546\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Topic\n",
    "\n",
    "test_labels = test_tweets[\"Relevance\"].tolist()\n",
    "test_topics = test_tweets[\"Topic Label\"].tolist()\n",
    "test_corpus = list(test_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, t) in zip(test_labels, test_corpus, test_topics):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[\"t\"] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Relevance Accuracy on test data: \", nltk.classify.accuracy(classifierRelevance3NB, labeled_test_data))\n",
    "versionRelevance3NBacc = nltk.classify.accuracy(classifierRelevance3NB, labeled_test_data)\n",
    "print(\"M Relevance Accuracy on test data: \", nltk.classify.accuracy(classifierRelevance3M, labeled_test_data))\n",
    "versionRelevance3Macc = nltk.classify.accuracy(classifierRelevance3M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32847ee",
   "metadata": {},
   "source": [
    "Without Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2116d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.741\n",
      "             2          -0.31988        0.846\n",
      "             3          -0.25227        0.929\n",
      "             4          -0.21327        0.953\n",
      "             5          -0.18695        0.966\n",
      "             6          -0.16757        0.976\n",
      "             7          -0.15250        0.979\n",
      "             8          -0.14035        0.984\n",
      "             9          -0.13028        0.986\n",
      "            10          -0.12177        0.987\n",
      "            11          -0.11447        0.988\n",
      "            12          -0.10811        0.989\n",
      "            13          -0.10252        0.990\n",
      "            14          -0.09756        0.990\n",
      "            15          -0.09313        0.991\n",
      "            16          -0.08913        0.992\n",
      "            17          -0.08551        0.992\n",
      "            18          -0.08222        0.992\n",
      "            19          -0.07920        0.993\n",
      "            20          -0.07643        0.993\n",
      "            21          -0.07387        0.994\n",
      "            22          -0.07150        0.994\n",
      "            23          -0.06930        0.994\n",
      "            24          -0.06725        0.994\n",
      "            25          -0.06534        0.994\n",
      "            26          -0.06355        0.994\n",
      "            27          -0.06186        0.994\n",
      "            28          -0.06028        0.994\n",
      "            29          -0.05879        0.995\n",
      "            30          -0.05738        0.995\n",
      "            31          -0.05605        0.995\n",
      "            32          -0.05479        0.995\n",
      "            33          -0.05359        0.995\n",
      "            34          -0.05245        0.995\n",
      "            35          -0.05137        0.995\n",
      "            36          -0.05034        0.995\n",
      "            37          -0.04935        0.995\n",
      "            38          -0.04841        0.995\n",
      "            39          -0.04751        0.995\n",
      "            40          -0.04665        0.995\n",
      "            41          -0.04583        0.995\n",
      "            42          -0.04503        0.995\n",
      "            43          -0.04427        0.995\n",
      "            44          -0.04354        0.995\n",
      "            45          -0.04284        0.995\n",
      "            46          -0.04216        0.995\n",
      "            47          -0.04151        0.995\n",
      "            48          -0.04088        0.995\n",
      "            49          -0.04028        0.995\n",
      "            50          -0.03969        0.995\n",
      "            51          -0.03913        0.995\n",
      "            52          -0.03858        0.996\n",
      "            53          -0.03806        0.996\n",
      "            54          -0.03754        0.996\n",
      "            55          -0.03705        0.996\n",
      "            56          -0.03657        0.996\n",
      "            57          -0.03610        0.996\n",
      "            58          -0.03565        0.996\n",
      "            59          -0.03522        0.996\n",
      "            60          -0.03479        0.996\n",
      "            61          -0.03438        0.996\n",
      "            62          -0.03398        0.996\n",
      "            63          -0.03359        0.996\n",
      "            64          -0.03321        0.996\n",
      "            65          -0.03284        0.996\n",
      "            66          -0.03248        0.996\n",
      "            67          -0.03213        0.996\n",
      "            68          -0.03179        0.996\n",
      "            69          -0.03146        0.996\n",
      "            70          -0.03114        0.996\n",
      "            71          -0.03082        0.996\n",
      "            72          -0.03051        0.996\n",
      "            73          -0.03021        0.996\n",
      "            74          -0.02992        0.996\n",
      "            75          -0.02964        0.996\n",
      "            76          -0.02936        0.996\n",
      "            77          -0.02908        0.996\n",
      "            78          -0.02882        0.996\n",
      "            79          -0.02856        0.996\n",
      "            80          -0.02830        0.996\n",
      "            81          -0.02805        0.996\n",
      "            82          -0.02781        0.996\n",
      "            83          -0.02757        0.996\n",
      "            84          -0.02734        0.996\n",
      "            85          -0.02711        0.996\n",
      "            86          -0.02688        0.996\n",
      "            87          -0.02666        0.996\n",
      "            88          -0.02645        0.997\n",
      "            89          -0.02624        0.997\n",
      "            90          -0.02603        0.997\n",
      "            91          -0.02583        0.997\n",
      "            92          -0.02563        0.997\n",
      "            93          -0.02544        0.997\n",
      "            94          -0.02525        0.997\n",
      "            95          -0.02506        0.997\n",
      "            96          -0.02487        0.997\n",
      "            97          -0.02469        0.997\n",
      "            98          -0.02452        0.997\n",
      "            99          -0.02434        0.997\n",
      "         Final          -0.02417        0.997\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Topic\n",
    "\n",
    "train_labels = train_tweets[\"Relevance\"].tolist()\n",
    "train_corpus = list(train_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s) in zip(train_labels, train_corpus):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifierRelevance4NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifierRelevance4M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a0bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Relevance Accuracy on test data:  0.543\n",
      "M Relevance Accuracy on test data:  0.568\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Topic\n",
    "\n",
    "test_labels = test_tweets[\"Relevance\"].tolist()\n",
    "test_corpus = list(test_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s) in zip(test_labels, test_corpus):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Relevance Accuracy on test data: \", nltk.classify.accuracy(classifierRelevance4NB, labeled_test_data))\n",
    "versionRelevance4NBacc = nltk.classify.accuracy(classifierRelevance4NB, labeled_test_data)\n",
    "print(\"M Relevance Accuracy on test data: \", nltk.classify.accuracy(classifierRelevance4M, labeled_test_data))\n",
    "versionRelevance4Macc = nltk.classify.accuracy(classifierRelevance4M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef1692b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Topic and Hashtags:  0.647\n",
      "Maxent with Topic and Hashtags:  0.769\n",
      "Naive Bayes with Hashtags:  0.706\n",
      "Maxent with Hashtags:  0.781\n",
      "Naive Bayes with Topic:  0.543\n",
      "Maxent with Topic:  0.546\n",
      "Naive Bayes without either:  0.543\n",
      "Maxent without either:  0.568\n"
     ]
    }
   ],
   "source": [
    "# Accuracies for Relevance\n",
    "\n",
    "print(\"Naive Bayes with Topic and Hashtags: \",versionRelevance1NBacc)\n",
    "print(\"Maxent with Topic and Hashtags: \",versionRelevance1Macc)\n",
    "print(\"Naive Bayes with Hashtags: \",versionRelevance2NBacc)\n",
    "print(\"Maxent with Hashtags: \",versionRelevance2Macc)\n",
    "print(\"Naive Bayes with Topic: \",versionRelevance3NBacc)\n",
    "print(\"Maxent with Topic: \",versionRelevance3Macc)\n",
    "print(\"Naive Bayes without either: \",versionRelevance4NBacc)\n",
    "print(\"Maxent without either: \",versionRelevance4Macc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be87e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('relevance_model.pkl', 'wb') as file: \n",
    "    # Serialize the data and write it to the file \n",
    "    pickle.dump(classifierRelevance2M, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90779fdc",
   "metadata": {},
   "source": [
    "# Getting Relevant Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8819bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_relevant_tweets = train_tweets[train_tweets[\"Relevance\"] == \"Relevant\"]\n",
    "test_relevant_tweets = test_tweets[test_tweets[\"Relevance\"] == \"Relevant\"]\n",
    "validate_relevant_tweets = validate_tweets[validate_tweets[\"Relevance\"] == \"Relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd65e29",
   "metadata": {},
   "source": [
    "# Naive Bayes and Maxent Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c145bad",
   "metadata": {},
   "source": [
    "With Topic and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "349804fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.231\n",
      "             2          -0.74410        0.848\n",
      "             3          -0.58586        0.902\n",
      "             4          -0.49051        0.925\n",
      "             5          -0.42581        0.940\n",
      "             6          -0.37840        0.952\n",
      "             7          -0.34181        0.959\n",
      "             8          -0.31253        0.966\n",
      "             9          -0.28845        0.969\n",
      "            10          -0.26823        0.971\n",
      "            11          -0.25097        0.973\n",
      "            12          -0.23603        0.974\n",
      "            13          -0.22295        0.976\n",
      "            14          -0.21139        0.976\n",
      "            15          -0.20110        0.978\n",
      "            16          -0.19186        0.978\n",
      "            17          -0.18353        0.979\n",
      "            18          -0.17596        0.981\n",
      "            19          -0.16905        0.981\n",
      "            20          -0.16273        0.982\n",
      "            21          -0.15691        0.983\n",
      "            22          -0.15153        0.983\n",
      "            23          -0.14656        0.984\n",
      "            24          -0.14193        0.984\n",
      "            25          -0.13762        0.984\n",
      "            26          -0.13360        0.985\n",
      "            27          -0.12983        0.987\n",
      "            28          -0.12629        0.987\n",
      "            29          -0.12297        0.988\n",
      "            30          -0.11983        0.988\n",
      "            31          -0.11687        0.988\n",
      "            32          -0.11408        0.988\n",
      "            33          -0.11143        0.988\n",
      "            34          -0.10891        0.988\n",
      "            35          -0.10652        0.988\n",
      "            36          -0.10425        0.989\n",
      "            37          -0.10208        0.989\n",
      "            38          -0.10002        0.989\n",
      "            39          -0.09805        0.989\n",
      "            40          -0.09616        0.989\n",
      "            41          -0.09436        0.989\n",
      "            42          -0.09263        0.989\n",
      "            43          -0.09097        0.989\n",
      "            44          -0.08938        0.989\n",
      "            45          -0.08785        0.989\n",
      "            46          -0.08638        0.989\n",
      "            47          -0.08497        0.990\n",
      "            48          -0.08361        0.990\n",
      "            49          -0.08230        0.990\n",
      "            50          -0.08103        0.990\n",
      "            51          -0.07981        0.990\n",
      "            52          -0.07864        0.990\n",
      "            53          -0.07750        0.990\n",
      "            54          -0.07640        0.991\n",
      "            55          -0.07533        0.991\n",
      "            56          -0.07430        0.991\n",
      "            57          -0.07330        0.991\n",
      "            58          -0.07234        0.991\n",
      "            59          -0.07140        0.991\n",
      "            60          -0.07049        0.991\n",
      "            61          -0.06961        0.991\n",
      "            62          -0.06875        0.991\n",
      "            63          -0.06792        0.991\n",
      "            64          -0.06711        0.991\n",
      "            65          -0.06632        0.991\n",
      "            66          -0.06556        0.991\n",
      "            67          -0.06481        0.991\n",
      "            68          -0.06409        0.991\n",
      "            69          -0.06338        0.991\n",
      "            70          -0.06270        0.991\n",
      "            71          -0.06203        0.991\n",
      "            72          -0.06137        0.991\n",
      "            73          -0.06074        0.991\n",
      "            74          -0.06012        0.991\n",
      "            75          -0.05951        0.991\n",
      "            76          -0.05892        0.991\n",
      "            77          -0.05834        0.991\n",
      "            78          -0.05778        0.991\n",
      "            79          -0.05723        0.991\n",
      "            80          -0.05669        0.991\n",
      "            81          -0.05617        0.991\n",
      "            82          -0.05565        0.991\n",
      "            83          -0.05515        0.991\n",
      "            84          -0.05466        0.991\n",
      "            85          -0.05418        0.991\n",
      "            86          -0.05371        0.991\n",
      "            87          -0.05325        0.991\n",
      "            88          -0.05280        0.991\n",
      "            89          -0.05235        0.991\n",
      "            90          -0.05192        0.991\n",
      "            91          -0.05150        0.991\n",
      "            92          -0.05108        0.991\n",
      "            93          -0.05067        0.992\n",
      "            94          -0.05027        0.992\n",
      "            95          -0.04988        0.992\n",
      "            96          -0.04950        0.992\n",
      "            97          -0.04912        0.992\n",
      "            98          -0.04875        0.992\n",
      "            99          -0.04839        0.992\n",
      "         Final          -0.04803        0.992\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Topic and Hashtags\n",
    "\n",
    "train_labels = train_relevant_tweets[\"Sentiment\"].tolist()\n",
    "train_hashtags = train_relevant_tweets[\"hashtags\"].tolist()\n",
    "train_hashtags = [str(hashtags).split(\",\") for hashtags in train_hashtags] \n",
    "train_topics = train_relevant_tweets[\"Topic Label\"].tolist()\n",
    "train_corpus = list(train_relevant_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s, t, h) in zip(train_labels, train_corpus, train_topics, train_hashtags):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifier1NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifier1M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd234890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy on test data:  0.3963963963963964\n",
      "M Accuracy on test data:  0.44015444015444016\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Topic and Hashtags\n",
    "\n",
    "test_labels = test_relevant_tweets[\"Sentiment\"].tolist()\n",
    "test_hashtags = test_relevant_tweets[\"hashtags\"].tolist()\n",
    "test_hashtags = [str(hashtags).split(\",\") for hashtags in test_hashtags] \n",
    "test_topics = test_relevant_tweets[\"Topic Label\"].tolist()\n",
    "test_corpus = list(test_relevant_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, t, h) in zip(test_labels, test_corpus, test_topics, test_hashtags):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Accuracy on test data: \", nltk.classify.accuracy(classifier1NB, labeled_test_data))\n",
    "version1NBacc = nltk.classify.accuracy(classifier1NB, labeled_test_data)\n",
    "print(\"M Accuracy on test data: \", nltk.classify.accuracy(classifier1M, labeled_test_data))\n",
    "version1Macc = nltk.classify.accuracy(classifier1M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443d325",
   "metadata": {},
   "source": [
    "With Hashtags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0a80420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.231\n",
      "             2          -0.72689        0.855\n",
      "             3          -0.56718        0.910\n",
      "             4          -0.47290        0.929\n",
      "             5          -0.40952        0.944\n",
      "             6          -0.36331        0.957\n",
      "             7          -0.32776        0.965\n",
      "             8          -0.29938        0.969\n",
      "             9          -0.27608        0.969\n",
      "            10          -0.25655        0.973\n",
      "            11          -0.23989        0.974\n",
      "            12          -0.22549        0.975\n",
      "            13          -0.21291        0.976\n",
      "            14          -0.20180        0.977\n",
      "            15          -0.19191        0.978\n",
      "            16          -0.18304        0.980\n",
      "            17          -0.17504        0.981\n",
      "            18          -0.16779        0.982\n",
      "            19          -0.16118        0.982\n",
      "            20          -0.15513        0.983\n",
      "            21          -0.14956        0.983\n",
      "            22          -0.14442        0.983\n",
      "            23          -0.13967        0.985\n",
      "            24          -0.13525        0.986\n",
      "            25          -0.13114        0.986\n",
      "            26          -0.12730        0.987\n",
      "            27          -0.12371        0.988\n",
      "            28          -0.12034        0.988\n",
      "            29          -0.11716        0.988\n",
      "            30          -0.11418        0.988\n",
      "            31          -0.11136        0.988\n",
      "            32          -0.10869        0.988\n",
      "            33          -0.10617        0.988\n",
      "            34          -0.10378        0.989\n",
      "            35          -0.10150        0.989\n",
      "            36          -0.09934        0.989\n",
      "            37          -0.09728        0.989\n",
      "            38          -0.09531        0.989\n",
      "            39          -0.09344        0.989\n",
      "            40          -0.09164        0.989\n",
      "            41          -0.08993        0.989\n",
      "            42          -0.08829        0.989\n",
      "            43          -0.08671        0.989\n",
      "            44          -0.08520        0.989\n",
      "            45          -0.08375        0.989\n",
      "            46          -0.08235        0.989\n",
      "            47          -0.08101        0.989\n",
      "            48          -0.07972        0.990\n",
      "            49          -0.07847        0.990\n",
      "            50          -0.07727        0.990\n",
      "            51          -0.07611        0.990\n",
      "            52          -0.07499        0.990\n",
      "            53          -0.07391        0.991\n",
      "            54          -0.07287        0.991\n",
      "            55          -0.07185        0.991\n",
      "            56          -0.07088        0.991\n",
      "            57          -0.06993        0.991\n",
      "            58          -0.06901        0.991\n",
      "            59          -0.06812        0.991\n",
      "            60          -0.06726        0.991\n",
      "            61          -0.06642        0.991\n",
      "            62          -0.06561        0.991\n",
      "            63          -0.06482        0.991\n",
      "            64          -0.06405        0.991\n",
      "            65          -0.06330        0.991\n",
      "            66          -0.06258        0.991\n",
      "            67          -0.06187        0.991\n",
      "            68          -0.06118        0.991\n",
      "            69          -0.06051        0.991\n",
      "            70          -0.05986        0.991\n",
      "            71          -0.05923        0.991\n",
      "            72          -0.05861        0.991\n",
      "            73          -0.05801        0.991\n",
      "            74          -0.05742        0.991\n",
      "            75          -0.05684        0.991\n",
      "            76          -0.05628        0.991\n",
      "            77          -0.05574        0.991\n",
      "            78          -0.05520        0.991\n",
      "            79          -0.05468        0.991\n",
      "            80          -0.05417        0.991\n",
      "            81          -0.05367        0.991\n",
      "            82          -0.05318        0.991\n",
      "            83          -0.05271        0.991\n",
      "            84          -0.05224        0.991\n",
      "            85          -0.05178        0.991\n",
      "            86          -0.05134        0.991\n",
      "            87          -0.05090        0.991\n",
      "            88          -0.05047        0.992\n",
      "            89          -0.05005        0.992\n",
      "            90          -0.04964        0.992\n",
      "            91          -0.04924        0.992\n",
      "            92          -0.04885        0.992\n",
      "            93          -0.04846        0.992\n",
      "            94          -0.04808        0.992\n",
      "            95          -0.04771        0.992\n",
      "            96          -0.04735        0.992\n",
      "            97          -0.04699        0.992\n",
      "            98          -0.04664        0.992\n",
      "            99          -0.04630        0.992\n",
      "         Final          -0.04596        0.992\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Hashtags\n",
    "\n",
    "train_labels = train_relevant_tweets[\"Sentiment\"].tolist()\n",
    "train_hashtags = train_relevant_tweets[\"hashtags\"].tolist()\n",
    "train_hashtags = [str(hashtags).split(\",\") for hashtags in train_hashtags] \n",
    "train_corpus = list(train_relevant_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s, h) in zip(train_labels, train_corpus, train_hashtags):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifier2NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifier2M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21b5d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy on test data:  0.38481338481338484\n",
      "M Accuracy on test data:  0.42342342342342343\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Hashtags\n",
    "\n",
    "test_labels = test_relevant_tweets[\"Sentiment\"].tolist()\n",
    "test_hashtags = test_relevant_tweets[\"hashtags\"].tolist()\n",
    "test_hashtags = [str(hashtags).split(\",\") for hashtags in test_hashtags] \n",
    "test_corpus = list(test_relevant_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, h) in zip(test_labels, test_corpus, test_hashtags):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Accuracy on test data: \", nltk.classify.accuracy(classifier2NB, labeled_test_data))\n",
    "version2NBacc = nltk.classify.accuracy(classifier2NB, labeled_test_data)\n",
    "print(\"M Accuracy on test data: \", nltk.classify.accuracy(classifier2M, labeled_test_data))\n",
    "version2Macc = nltk.classify.accuracy(classifier2M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f3a13",
   "metadata": {},
   "source": [
    "With Topic Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "459b0c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.231\n",
      "             2          -0.76351        0.837\n",
      "             3          -0.61025        0.896\n",
      "             4          -0.51639        0.919\n",
      "             5          -0.45183        0.936\n",
      "             6          -0.40403        0.948\n",
      "             7          -0.36685        0.952\n",
      "             8          -0.33690        0.958\n",
      "             9          -0.31215        0.961\n",
      "            10          -0.29128        0.964\n",
      "            11          -0.27340        0.966\n",
      "            12          -0.25788        0.966\n",
      "            13          -0.24426        0.968\n",
      "            14          -0.23220        0.970\n",
      "            15          -0.22143        0.970\n",
      "            16          -0.21176        0.971\n",
      "            17          -0.20301        0.972\n",
      "            18          -0.19505        0.974\n",
      "            19          -0.18778        0.974\n",
      "            20          -0.18111        0.976\n",
      "            21          -0.17496        0.978\n",
      "            22          -0.16928        0.978\n",
      "            23          -0.16401        0.978\n",
      "            24          -0.15911        0.979\n",
      "            25          -0.15454        0.979\n",
      "            26          -0.15026        0.979\n",
      "            27          -0.14625        0.980\n",
      "            28          -0.14248        0.980\n",
      "            29          -0.13893        0.981\n",
      "            30          -0.13559        0.981\n",
      "            31          -0.13242        0.983\n",
      "            32          -0.12943        0.983\n",
      "            33          -0.12659        0.984\n",
      "            34          -0.12389        0.985\n",
      "            35          -0.12133        0.985\n",
      "            36          -0.11888        0.985\n",
      "            37          -0.11656        0.985\n",
      "            38          -0.11433        0.985\n",
      "            39          -0.11221        0.985\n",
      "            40          -0.11018        0.985\n",
      "            41          -0.10823        0.986\n",
      "            42          -0.10636        0.986\n",
      "            43          -0.10457        0.986\n",
      "            44          -0.10285        0.987\n",
      "            45          -0.10120        0.987\n",
      "            46          -0.09961        0.987\n",
      "            47          -0.09807        0.987\n",
      "            48          -0.09660        0.987\n",
      "            49          -0.09517        0.987\n",
      "            50          -0.09380        0.987\n",
      "            51          -0.09247        0.987\n",
      "            52          -0.09119        0.988\n",
      "            53          -0.08995        0.988\n",
      "            54          -0.08875        0.988\n",
      "            55          -0.08758        0.988\n",
      "            56          -0.08646        0.988\n",
      "            57          -0.08537        0.988\n",
      "            58          -0.08431        0.988\n",
      "            59          -0.08328        0.988\n",
      "            60          -0.08229        0.988\n",
      "            61          -0.08132        0.988\n",
      "            62          -0.08038        0.988\n",
      "            63          -0.07947        0.988\n",
      "            64          -0.07858        0.988\n",
      "            65          -0.07771        0.988\n",
      "            66          -0.07687        0.988\n",
      "            67          -0.07605        0.988\n",
      "            68          -0.07526        0.988\n",
      "            69          -0.07448        0.988\n",
      "            70          -0.07372        0.988\n",
      "            71          -0.07298        0.988\n",
      "            72          -0.07226        0.988\n",
      "            73          -0.07156        0.988\n",
      "            74          -0.07087        0.988\n",
      "            75          -0.07020        0.988\n",
      "            76          -0.06955        0.988\n",
      "            77          -0.06891        0.988\n",
      "            78          -0.06828        0.988\n",
      "            79          -0.06767        0.988\n",
      "            80          -0.06708        0.988\n",
      "            81          -0.06649        0.988\n",
      "            82          -0.06592        0.988\n",
      "            83          -0.06536        0.988\n",
      "            84          -0.06481        0.988\n",
      "            85          -0.06428        0.988\n",
      "            86          -0.06375        0.988\n",
      "            87          -0.06324        0.988\n",
      "            88          -0.06273        0.988\n",
      "            89          -0.06224        0.989\n",
      "            90          -0.06176        0.989\n",
      "            91          -0.06128        0.989\n",
      "            92          -0.06082        0.989\n",
      "            93          -0.06036        0.989\n",
      "            94          -0.05991        0.989\n",
      "            95          -0.05947        0.989\n",
      "            96          -0.05904        0.989\n",
      "            97          -0.05862        0.989\n",
      "            98          -0.05820        0.989\n",
      "            99          -0.05780        0.989\n",
      "         Final          -0.05739        0.989\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Topic\n",
    "\n",
    "train_labels = train_relevant_tweets[\"Sentiment\"].tolist()\n",
    "train_topics = train_relevant_tweets[\"Topic Label\"].tolist()\n",
    "train_corpus = list(train_relevant_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s, t) in zip(train_labels, train_corpus, train_topics):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifier3NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifier3M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfc6a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy on test data:  0.31917631917631917\n",
      "M Accuracy on test data:  0.3410553410553411\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Topic\n",
    "\n",
    "test_labels = test_relevant_tweets[\"Sentiment\"].tolist()\n",
    "test_topics = test_relevant_tweets[\"Topic Label\"].tolist()\n",
    "test_corpus = list(test_relevant_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, t) in zip(test_labels, test_corpus, test_topics):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Accuracy on test data: \", nltk.classify.accuracy(classifier3NB, labeled_test_data))\n",
    "version3NBacc = nltk.classify.accuracy(classifier3NB, labeled_test_data)\n",
    "print(\"M Accuracy on test data: \", nltk.classify.accuracy(classifier3M, labeled_test_data))\n",
    "version3Macc = nltk.classify.accuracy(classifier3M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84515590",
   "metadata": {},
   "source": [
    "Without Either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4499542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.231\n",
      "             2          -0.74311        0.853\n",
      "             3          -0.58752        0.903\n",
      "             4          -0.49456        0.927\n",
      "             5          -0.43138        0.939\n",
      "             6          -0.38492        0.949\n",
      "             7          -0.34895        0.955\n",
      "             8          -0.32007        0.960\n",
      "             9          -0.29627        0.962\n",
      "            10          -0.27625        0.964\n",
      "            11          -0.25912        0.966\n",
      "            12          -0.24429        0.967\n",
      "            13          -0.23128        0.969\n",
      "            14          -0.21978        0.971\n",
      "            15          -0.20953        0.972\n",
      "            16          -0.20032        0.973\n",
      "            17          -0.19200        0.974\n",
      "            18          -0.18445        0.977\n",
      "            19          -0.17755        0.978\n",
      "            20          -0.17122        0.980\n",
      "            21          -0.16540        0.980\n",
      "            22          -0.16002        0.980\n",
      "            23          -0.15503        0.980\n",
      "            24          -0.15039        0.981\n",
      "            25          -0.14606        0.981\n",
      "            26          -0.14202        0.982\n",
      "            27          -0.13823        0.982\n",
      "            28          -0.13467        0.982\n",
      "            29          -0.13132        0.983\n",
      "            30          -0.12816        0.983\n",
      "            31          -0.12517        0.984\n",
      "            32          -0.12234        0.984\n",
      "            33          -0.11967        0.984\n",
      "            34          -0.11712        0.984\n",
      "            35          -0.11470        0.984\n",
      "            36          -0.11240        0.984\n",
      "            37          -0.11021        0.984\n",
      "            38          -0.10811        0.984\n",
      "            39          -0.10611        0.985\n",
      "            40          -0.10420        0.985\n",
      "            41          -0.10236        0.985\n",
      "            42          -0.10061        0.985\n",
      "            43          -0.09892        0.985\n",
      "            44          -0.09730        0.985\n",
      "            45          -0.09574        0.985\n",
      "            46          -0.09424        0.986\n",
      "            47          -0.09280        0.986\n",
      "            48          -0.09141        0.987\n",
      "            49          -0.09007        0.988\n",
      "            50          -0.08878        0.988\n",
      "            51          -0.08753        0.988\n",
      "            52          -0.08632        0.988\n",
      "            53          -0.08516        0.988\n",
      "            54          -0.08403        0.988\n",
      "            55          -0.08294        0.988\n",
      "            56          -0.08188        0.988\n",
      "            57          -0.08085        0.988\n",
      "            58          -0.07986        0.988\n",
      "            59          -0.07889        0.988\n",
      "            60          -0.07796        0.988\n",
      "            61          -0.07705        0.988\n",
      "            62          -0.07616        0.988\n",
      "            63          -0.07530        0.988\n",
      "            64          -0.07447        0.988\n",
      "            65          -0.07366        0.988\n",
      "            66          -0.07287        0.988\n",
      "            67          -0.07210        0.988\n",
      "            68          -0.07135        0.988\n",
      "            69          -0.07062        0.988\n",
      "            70          -0.06990        0.988\n",
      "            71          -0.06921        0.988\n",
      "            72          -0.06853        0.988\n",
      "            73          -0.06787        0.988\n",
      "            74          -0.06723        0.988\n",
      "            75          -0.06660        0.989\n",
      "            76          -0.06598        0.989\n",
      "            77          -0.06538        0.989\n",
      "            78          -0.06480        0.989\n",
      "            79          -0.06422        0.989\n",
      "            80          -0.06366        0.989\n",
      "            81          -0.06311        0.989\n",
      "            82          -0.06258        0.989\n",
      "            83          -0.06205        0.989\n",
      "            84          -0.06154        0.989\n",
      "            85          -0.06103        0.989\n",
      "            86          -0.06054        0.989\n",
      "            87          -0.06006        0.989\n",
      "            88          -0.05958        0.989\n",
      "            89          -0.05912        0.989\n",
      "            90          -0.05867        0.989\n",
      "            91          -0.05822        0.989\n",
      "            92          -0.05778        0.989\n",
      "            93          -0.05735        0.989\n",
      "            94          -0.05693        0.989\n",
      "            95          -0.05652        0.989\n",
      "            96          -0.05612        0.989\n",
      "            97          -0.05572        0.989\n",
      "            98          -0.05533        0.989\n",
      "            99          -0.05495        0.989\n",
      "         Final          -0.05457        0.989\n"
     ]
    }
   ],
   "source": [
    "# Train Data with Topic\n",
    "\n",
    "train_labels = train_relevant_tweets[\"Sentiment\"].tolist()\n",
    "train_corpus = list(train_relevant_tweets[\"token\"])\n",
    "train_corpus = [eval(i) for i in train_corpus] \n",
    "\n",
    "train_dictionary = gensim.corpora.Dictionary(train_corpus)\n",
    "\n",
    "labeled_training_data = []\n",
    "for (l, s) in zip(train_labels, train_corpus):\n",
    "    vector = train_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    labeled_training_data.append((sent_as_dict, l))\n",
    "    \n",
    "classifier4NB = nltk.NaiveBayesClassifier.train(labeled_training_data)\n",
    "classifier4M = nltk.MaxentClassifier.train(labeled_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ed6ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy on test data:  0.323037323037323\n",
      "M Accuracy on test data:  0.32432432432432434\n"
     ]
    }
   ],
   "source": [
    "# Test Data with Topic\n",
    "\n",
    "test_labels = test_relevant_tweets[\"Sentiment\"].tolist()\n",
    "test_corpus = list(test_relevant_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s) in zip(test_labels, test_corpus):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "    \n",
    "print(\"NB Accuracy on test data: \", nltk.classify.accuracy(classifier4NB, labeled_test_data))\n",
    "version4NBacc = nltk.classify.accuracy(classifier4NB, labeled_test_data)\n",
    "print(\"M Accuracy on test data: \", nltk.classify.accuracy(classifier4M, labeled_test_data))\n",
    "version4Macc = nltk.classify.accuracy(classifier4M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3b2d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Topic and Hashtags:  0.3963963963963964\n",
      "Maxent with Topic and Hashtags:  0.44015444015444016\n",
      "Naive Bayes with Hashtags:  0.38481338481338484\n",
      "Maxent with Hashtags:  0.42342342342342343\n",
      "Naive Bayes with Topic:  0.31917631917631917\n",
      "Maxent with Topic:  0.3410553410553411\n",
      "Naive Bayes without either:  0.323037323037323\n",
      "Maxent without either:  0.32432432432432434\n"
     ]
    }
   ],
   "source": [
    "# Accuracies\n",
    "\n",
    "print(\"Naive Bayes with Topic and Hashtags: \",version1NBacc)\n",
    "print(\"Maxent with Topic and Hashtags: \",version1Macc)\n",
    "print(\"Naive Bayes with Hashtags: \",version2NBacc)\n",
    "print(\"Maxent with Hashtags: \",version2Macc)\n",
    "print(\"Naive Bayes with Topic: \",version3NBacc)\n",
    "print(\"Maxent with Topic: \",version3Macc)\n",
    "print(\"Naive Bayes without either: \",version4NBacc)\n",
    "print(\"Maxent without either: \",version4Macc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92c25b",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "951b8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy on validation data:  0.4426229508196721\n",
      "M Accuracy on validation data:  0.49453551912568305\n"
     ]
    }
   ],
   "source": [
    "# Validation Data with Topic and Hashtags\n",
    "\n",
    "test_labels = validate_relevant_tweets[\"Sentiment\"].tolist()\n",
    "test_hashtags = validate_relevant_tweets[\"hashtags\"].tolist()\n",
    "test_hashtags = [str(hashtags).split(\",\") for hashtags in test_hashtags] \n",
    "test_topics = validate_relevant_tweets[\"Topic Label\"].tolist()\n",
    "test_corpus = list(validate_relevant_tweets[\"token\"])\n",
    "test_corpus = [eval(i) for i in test_corpus] \n",
    "\n",
    "test_dictionary = gensim.corpora.Dictionary(test_corpus)\n",
    "\n",
    "labeled_test_data = []\n",
    "for (l, s, t, h) in zip(test_labels, test_corpus, test_topics, test_hashtags):\n",
    "    vector = test_dictionary.doc2bow(s)\n",
    "    sent_as_dict = {id:1 for (id, tf) in vector}\n",
    "    sent_as_dict[t] = 1\n",
    "    for tag in h:\n",
    "        sent_as_dict[tag] = 1\n",
    "    labeled_test_data.append((sent_as_dict, l))\n",
    "\n",
    "print(\"NB Accuracy on validation data: \", nltk.classify.accuracy(classifier1NB, labeled_test_data))\n",
    "version1NBacc = nltk.classify.accuracy(classifier1NB, labeled_test_data)\n",
    "print(\"M Accuracy on validation data: \", nltk.classify.accuracy(classifier1M, labeled_test_data))\n",
    "version1Macc = nltk.classify.accuracy(classifier1M, labeled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sentiment_model.pkl', 'wb') as file: \n",
    "    # Serialize the data and write it to the file \n",
    "    pickle.dump(classifier1M, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b380ac6",
   "metadata": {},
   "source": [
    "# EmoRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b800e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7d397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained emotion classification model\n",
    "emorobertamodel = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37dad97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (5000) does not match length of index (494540)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tweet_text \u001b[38;5;241m=\u001b[39m labelled_tweets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      2\u001b[0m emotions \u001b[38;5;241m=\u001b[39m emorobertamodel(tweet_text)\n\u001b[1;32m----> 3\u001b[0m tweets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m emotions]\n\u001b[0;32m      4\u001b[0m tweets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m emotions]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4166\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4172\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4174\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4177\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4178\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4179\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4180\u001b[0m     ):\n\u001b[0;32m   4181\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4915\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (5000) does not match length of index (494540)"
     ]
    }
   ],
   "source": [
    "tweet_text = labelled_tweets[\"text\"].tolist()\n",
    "emotions = emorobertamodel(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad222c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"emotion_label\"] = \"\"\n",
    "tweets[\"emotion_score\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa3e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99zhe\\AppData\\Local\\Temp\\ipykernel_44372\\692127230.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets[\"emotion_label\"].iloc[0:5000] = [d[\"label\"] for d in emotions]\n",
      "C:\\Users\\99zhe\\AppData\\Local\\Temp\\ipykernel_44372\\692127230.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets[\"emotion_score\"].iloc[0:5000] = [d[\"score\"] for d in emotions]\n"
     ]
    }
   ],
   "source": [
    "emotions\n",
    "tweets[\"emotion_label\"].iloc[0:5000] = [d[\"label\"] for d in emotions]\n",
    "tweets[\"emotion_score\"].iloc[0:5000] = [d[\"score\"] for d in emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86e92cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>token</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>Topic Weight</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>emotion_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.467372e+07</td>\n",
       "      <td>1.627620e+18</td>\n",
       "      <td>repdavid i support 100</td>\n",
       "      <td>SlavaUkraïni,JoeBiden</td>\n",
       "      <td>['repdavid', 'support']</td>\n",
       "      <td>repdavid support</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.389973</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.520785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.282984e+09</td>\n",
       "      <td>1.611400e+18</td>\n",
       "      <td>province editorial inequality hounds s populat...</td>\n",
       "      <td>Canada,disabled,Vancouver,homeless</td>\n",
       "      <td>['province', 'editorial', 'inequality', 'hound...</td>\n",
       "      <td>province editorial inequality hound population...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.464470</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.661632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.830324e+08</td>\n",
       "      <td>1.636650e+18</td>\n",
       "      <td>they went to war when russia invaded ukraine t...</td>\n",
       "      <td>ArmUkraineNow</td>\n",
       "      <td>['went', 'war', 'russia', 'invaded', 'ukraine'...</td>\n",
       "      <td>went war russia invaded ukraine believed freed...</td>\n",
       "      <td>Strategic Military Aspects</td>\n",
       "      <td>0.613352</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.605659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.257760e+18</td>\n",
       "      <td>1.626920e+18</td>\n",
       "      <td>on mintwave radio mint2dry</td>\n",
       "      <td>nowplaying,alexa,streema,Scotland,echo,global,...</td>\n",
       "      <td>['mintwave', 'radio', 'mintdry']</td>\n",
       "      <td>mintwave radio mintdry</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.862803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.241172e+08</td>\n",
       "      <td>1.640020e+18</td>\n",
       "      <td>military historian douglas macgregor why will ...</td>\n",
       "      <td>NATO,Russia,Kiev</td>\n",
       "      <td>['military', 'historian', 'douglas', 'macgrego...</td>\n",
       "      <td>military historian douglas macgregor crumble d...</td>\n",
       "      <td>Broad Overview of the Conflict</td>\n",
       "      <td>0.434376</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.826465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494535</th>\n",
       "      <td>1.327670e+18</td>\n",
       "      <td>1.638460e+18</td>\n",
       "      <td>as a result of this nights attack on kyiv regi...</td>\n",
       "      <td>RussiaIsATerroristState,ArmUkraineNow</td>\n",
       "      <td>['result', 'night', 'attack', 'kyiv', 'region'...</td>\n",
       "      <td>result night attack kyiv region russian shelle...</td>\n",
       "      <td>Military Actions and Operations</td>\n",
       "      <td>0.485424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494536</th>\n",
       "      <td>3.915855e+08</td>\n",
       "      <td>1.659220e+18</td>\n",
       "      <td>computools has been recognized as one of the t...</td>\n",
       "      <td>development,mobileappdevelopment,canada,services</td>\n",
       "      <td>['computools', 'recognized', 'mobile', 'app', ...</td>\n",
       "      <td>computools recognized mobile app developer can...</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.394642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494537</th>\n",
       "      <td>1.001233e+08</td>\n",
       "      <td>1.635110e+18</td>\n",
       "      <td>good morning remember to wave 👋 when new traff...</td>\n",
       "      <td>NAFOfellas,SlavaUkraini,СлаваУкраїні,RussiaisA...</td>\n",
       "      <td>['good', 'morning', 'remember', 'wave', 'new',...</td>\n",
       "      <td>good morning remember wave new traffic partici...</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.483856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494538</th>\n",
       "      <td>1.534910e+18</td>\n",
       "      <td>1.619070e+18</td>\n",
       "      <td>colonial trope forces german apology germany h...</td>\n",
       "      <td>germany,Ukraine,AfricanStream,africa</td>\n",
       "      <td>['colonial', 'trope', 'force', 'german', 'apol...</td>\n",
       "      <td>colonial trope force german apology germany ap...</td>\n",
       "      <td>International Support and Public Opinion</td>\n",
       "      <td>0.309035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494539</th>\n",
       "      <td>7.257840e+17</td>\n",
       "      <td>1.637870e+18</td>\n",
       "      <td>former republican governor larry hogan respond...</td>\n",
       "      <td>StandWithUkraine</td>\n",
       "      <td>['republican', 'governor', 'larry', 'hogan', '...</td>\n",
       "      <td>republican governor larry hogan responds ron d...</td>\n",
       "      <td>Civilian Support and Information Sharing</td>\n",
       "      <td>0.427639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494540 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              userid       tweetid  \\\n",
       "0       5.467372e+07  1.627620e+18   \n",
       "1       2.282984e+09  1.611400e+18   \n",
       "2       2.830324e+08  1.636650e+18   \n",
       "3       1.257760e+18  1.626920e+18   \n",
       "4       9.241172e+08  1.640020e+18   \n",
       "...              ...           ...   \n",
       "494535  1.327670e+18  1.638460e+18   \n",
       "494536  3.915855e+08  1.659220e+18   \n",
       "494537  1.001233e+08  1.635110e+18   \n",
       "494538  1.534910e+18  1.619070e+18   \n",
       "494539  7.257840e+17  1.637870e+18   \n",
       "\n",
       "                                                     text  \\\n",
       "0                                 repdavid i support 100    \n",
       "1       province editorial inequality hounds s populat...   \n",
       "2       they went to war when russia invaded ukraine t...   \n",
       "3                              on mintwave radio mint2dry   \n",
       "4       military historian douglas macgregor why will ...   \n",
       "...                                                   ...   \n",
       "494535  as a result of this nights attack on kyiv regi...   \n",
       "494536  computools has been recognized as one of the t...   \n",
       "494537  good morning remember to wave 👋 when new traff...   \n",
       "494538  colonial trope forces german apology germany h...   \n",
       "494539  former republican governor larry hogan respond...   \n",
       "\n",
       "                                                 hashtags  \\\n",
       "0                                   SlavaUkraïni,JoeBiden   \n",
       "1                      Canada,disabled,Vancouver,homeless   \n",
       "2                                           ArmUkraineNow   \n",
       "3       nowplaying,alexa,streema,Scotland,echo,global,...   \n",
       "4                                        NATO,Russia,Kiev   \n",
       "...                                                   ...   \n",
       "494535              RussiaIsATerroristState,ArmUkraineNow   \n",
       "494536   development,mobileappdevelopment,canada,services   \n",
       "494537  NAFOfellas,SlavaUkraini,СлаваУкраїні,RussiaisA...   \n",
       "494538               germany,Ukraine,AfricanStream,africa   \n",
       "494539                                   StandWithUkraine   \n",
       "\n",
       "                                                    token  \\\n",
       "0                                 ['repdavid', 'support']   \n",
       "1       ['province', 'editorial', 'inequality', 'hound...   \n",
       "2       ['went', 'war', 'russia', 'invaded', 'ukraine'...   \n",
       "3                        ['mintwave', 'radio', 'mintdry']   \n",
       "4       ['military', 'historian', 'douglas', 'macgrego...   \n",
       "...                                                   ...   \n",
       "494535  ['result', 'night', 'attack', 'kyiv', 'region'...   \n",
       "494536  ['computools', 'recognized', 'mobile', 'app', ...   \n",
       "494537  ['good', 'morning', 'remember', 'wave', 'new',...   \n",
       "494538  ['colonial', 'trope', 'force', 'german', 'apol...   \n",
       "494539  ['republican', 'governor', 'larry', 'hogan', '...   \n",
       "\n",
       "                                           processed_text  \\\n",
       "0                                        repdavid support   \n",
       "1       province editorial inequality hound population...   \n",
       "2       went war russia invaded ukraine believed freed...   \n",
       "3                                  mintwave radio mintdry   \n",
       "4       military historian douglas macgregor crumble d...   \n",
       "...                                                   ...   \n",
       "494535  result night attack kyiv region russian shelle...   \n",
       "494536  computools recognized mobile app developer can...   \n",
       "494537  good morning remember wave new traffic partici...   \n",
       "494538  colonial trope force german apology germany ap...   \n",
       "494539  republican governor larry hogan responds ron d...   \n",
       "\n",
       "                                     Topic Label  Topic Weight   Sentiment  \\\n",
       "0       International Support and Public Opinion      0.389973    Positive   \n",
       "1                     Strategic Military Aspects      0.464470  Irrelevant   \n",
       "2                     Strategic Military Aspects      0.613352    Positive   \n",
       "3                Military Actions and Operations      0.279435  Irrelevant   \n",
       "4                 Broad Overview of the Conflict      0.434376    Negative   \n",
       "...                                          ...           ...         ...   \n",
       "494535           Military Actions and Operations      0.485424         NaN   \n",
       "494536  International Support and Public Opinion      0.394642         NaN   \n",
       "494537  International Support and Public Opinion      0.483856         NaN   \n",
       "494538  International Support and Public Opinion      0.309035         NaN   \n",
       "494539  Civilian Support and Information Sharing      0.427639         NaN   \n",
       "\n",
       "         Relevance emotion_label emotion_score  \n",
       "0         Relevant           joy      0.520785  \n",
       "1       Irrelevant         anger      0.661632  \n",
       "2         Relevant         anger      0.605659  \n",
       "3       Irrelevant       neutral      0.862803  \n",
       "4         Relevant          fear      0.826465  \n",
       "...            ...           ...           ...  \n",
       "494535  Irrelevant                              \n",
       "494536  Irrelevant                              \n",
       "494537  Irrelevant                              \n",
       "494538  Irrelevant                              \n",
       "494539  Irrelevant                              \n",
       "\n",
       "[494540 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e598da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"emotion_tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
